{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from numpy import asarray\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import scipy.misc\n",
    "from matplotlib import pyplot\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH=\"test2.jpg\"\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "def image_to_array(img):\n",
    "    return np.array(img)\n",
    "def array_to_image(arr):\n",
    "    return Image.fromarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_haar(img,face_cascade=face_cascade, required_size=(160, 160)):\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    images=[]\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        rgb = Image.fromarray(roi).resize(required_size)\n",
    "        images.append(rgb)\n",
    "    return images\n",
    "\n",
    "\n",
    "img=cv2.imread(IMAGE_PATH,1)\n",
    "extracted_faces=extract_faces_haar(img)\n",
    "\n",
    "\n",
    "def extract_face_haar(img,face_cascade=face_cascade, required_size=(160, 160)):\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    (x,y,w,h) = faces[0]\n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    rgb = Image.fromarray(roi).resize(required_size) \n",
    "    return image_to_array(rgb)\n",
    "\n",
    "    \n",
    "img=cv2.imread(IMAGE_PATH,1)\n",
    "extracted_face=extract_face_haar(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 13,  12,  17],\n",
       "        [ 13,  12,  16],\n",
       "        [ 11,  12,  14],\n",
       "        ...,\n",
       "        [ 19,  17,  30],\n",
       "        [ 20,  19,  32],\n",
       "        [ 20,  20,  32]],\n",
       "\n",
       "       [[ 13,  12,  17],\n",
       "        [ 13,  12,  16],\n",
       "        [ 11,  11,  14],\n",
       "        ...,\n",
       "        [ 19,  17,  30],\n",
       "        [ 20,  19,  32],\n",
       "        [ 20,  20,  32]],\n",
       "\n",
       "       [[ 12,  11,  16],\n",
       "        [ 12,  11,  15],\n",
       "        [  9,  10,  13],\n",
       "        ...,\n",
       "        [ 19,  18,  31],\n",
       "        [ 20,  19,  32],\n",
       "        [ 20,  20,  32]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 21,  20,  34],\n",
       "        [ 21,  20,  34],\n",
       "        [ 20,  18,  33],\n",
       "        ...,\n",
       "        [ 85,  75,  79],\n",
       "        [ 68,  58,  67],\n",
       "        [ 60,  50,  60]],\n",
       "\n",
       "       [[ 22,  21,  35],\n",
       "        [ 22,  21,  35],\n",
       "        [ 21,  20,  34],\n",
       "        ...,\n",
       "        [114, 103, 112],\n",
       "        [ 96,  85,  97],\n",
       "        [ 87,  77,  90]],\n",
       "\n",
       "       [[ 22,  21,  35],\n",
       "        [ 22,  21,  35],\n",
       "        [ 21,  20,  34],\n",
       "        ...,\n",
       "        [128, 117, 127],\n",
       "        [109,  98, 112],\n",
       "        [100,  89, 105]]], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(extracted_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faces_in_dir(path):\n",
    "    faces=[]\n",
    "    for i in listdir(path):\n",
    "        imgpath=path+i\n",
    "        img=cv2.imread(imgpath,1)\n",
    "        face=extract_face_haar(img)\n",
    "        faces.append(faces)\n",
    "    return faces\n",
    "val=load_faces_in_dir(\"5-celebrity-faces-dataset/train/ben_afflek/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(imagearray, required_size=(160, 160)):\n",
    "    detector = MTCNN()\n",
    "    results = detector.detect_faces(imagearray)\n",
    "    boxes=[]\n",
    "    for x in results:\n",
    "        boxes.append(x[\"box\"])\n",
    "    faces=[]\n",
    "    for (x,y,w,h) in boxes:\n",
    "        x1, y1 = abs(x), abs(y)\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        face = imagearray[y1:y2, x1:x2]\n",
    "        image = Image.fromarray(face)\n",
    "        image = image.resize(required_size)\n",
    "        faces.append(image)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faces(directory):\n",
    "\tfaces = list()\n",
    "\t# enumerate files\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + filename\n",
    "\t\t# get face\n",
    "\t\tface = extract_face(path)\n",
    "\t\t# store\n",
    "\t\tfaces.append(face)\n",
    "\treturn faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded 14 examples for class: ben_afflek\n",
      ">loaded 17 examples for class: elton_john\n",
      ">loaded 21 examples for class: jerry_seinfeld\n",
      ">loaded 19 examples for class: madonna\n",
      ">loaded 22 examples for class: mindy_kaling\n",
      "(93, 160, 160, 3) (93,)\n",
      ">loaded 5 examples for class: ben_afflek\n",
      ">loaded 5 examples for class: elton_john\n",
      ">loaded 5 examples for class: jerry_seinfeld\n",
      ">loaded 5 examples for class: madonna\n",
      ">loaded 5 examples for class: mindy_kaling\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    " \n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "\t# load image from file\n",
    "\timage = Image.open(filename)\n",
    "\t# convert to RGB, if needed\n",
    "\timage = image.convert('RGB')\n",
    "\t# convert to array\n",
    "\tpixels = asarray(image)\n",
    "\t# create the detector, using default weights\n",
    "\tdetector = MTCNN()\n",
    "\t# detect faces in the image\n",
    "\tresults = detector.detect_faces(pixels)\n",
    "\t# extract the bounding box from the first face\n",
    "\tx1, y1, width, height = results[0]['box']\n",
    "\t# bug fix\n",
    "\tx1, y1 = abs(x1), abs(y1)\n",
    "\tx2, y2 = x1 + width, y1 + height\n",
    "\t# extract the face\n",
    "\tface = pixels[y1:y2, x1:x2]\n",
    "\t# resize pixels to the model size\n",
    "\timage = Image.fromarray(face)\n",
    "\timage = image.resize(required_size)\n",
    "\tface_array = asarray(image)\n",
    "\treturn face_array\n",
    " \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "\tfaces = list()\n",
    "\t# enumerate files\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + filename\n",
    "\t\t# get face\n",
    "\t\tface = extract_face(path)\n",
    "\t\t# store\n",
    "\t\tfaces.append(face)\n",
    "\treturn faces\n",
    " \n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "\tX, y = list(), list()\n",
    "\t# enumerate folders, on per class\n",
    "\tfor subdir in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + subdir + '/'\n",
    "\t\t# skip any files that might be in the dir\n",
    "\t\tif not isdir(path):\n",
    "\t\t\tcontinue\n",
    "\t\t# load all faces in the subdirectory\n",
    "\t\tfaces = load_faces(path)\n",
    "\t\t# create labels\n",
    "\t\tlabels = [subdir for _ in range(len(faces))]\n",
    "\t\t# summarize progress\n",
    "\t\tprint('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "\t\t# store\n",
    "\t\tX.extend(faces)\n",
    "\t\ty.extend(labels)\n",
    "\treturn asarray(X), asarray(y)\n",
    " \n",
    "# load train dataset\n",
    "trainX, trainy = load_dataset('5-celebrity-faces-dataset/train/')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "testX, testy = load_dataset('5-celebrity-faces-dataset/val/')\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('5-celebrity-faces-dataset.npz', trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37764bittfconda9f6fabe4abc84d6ab4b66cd2d6c6be39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
